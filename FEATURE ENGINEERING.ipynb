{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZ8G1WwKTp/OoKMVhqltCc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["A parameter is a variable or a value that is used to define or influence the behavior of a system, function, model, or process. The specific meaning of \"parameter\" can vary depending on the context. Here are a few examples:\n","\n","1. In Mathematics and Functions:\n","\n","A parameter is a constant value that defines a family of functions. For instance, in the equation of a line\n","ùë¶\n","=\n","ùëö\n","ùë•\n","+\n","ùëè\n","y=mx+b, the slope\n","ùëö\n","m and the y-intercept\n","ùëè\n","b are parameters. Changing these parameters would result in different lines.\n","In functions, parameters are inputs that define how the function behaves. For example, in a function like\n","ùëì\n","(\n","ùë•\n",")\n","=\n","2\n","ùë•\n","+\n","3\n","f(x)=2x+3,\n","2\n","2 and\n","3\n","3 can be seen as parameters of the linear function.\n","2. In Programming:\n","\n","A parameter is a variable used in a function or method definition. It allows the function to accept values when called, making it more flexible and reusable.\n","For example, in a Python function:\n","\n","def add(a, b):\n","    return a + b\n","Here, a and b are parameters of the add function. When you call add(2, 3), the values 2 and 3 are arguments that are passed to these parameters.\n","3. In Statistics:\n","\n","A parameter is a numerical characteristic of a population (e.g., the mean, variance, or proportion). For example, the population mean\n","ùúá\n","Œº or population standard deviation\n","ùúé\n","œÉ are parameters that describe the overall distribution of data.\n","4. In Machine Learning and AI:\n","\n","Parameters refer to the internal variables of a model that are learned from data, such as the weights in a neural network. These parameters are adjusted during training to minimize the error between the model's predictions and the actual outcomes.\n","5. In System Design:\n","\n","Parameters are factors or settings that define the operation of a system or process. For example, the parameters for a search engine might include keywords, location, or time range."],"metadata":{"id":"am9iAfJPXbqJ"}},{"cell_type":"markdown","source":["Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. It tells us whether and how two variables move together. In other words, correlation indicates whether an increase in one variable is associated with an increase or decrease in another variable.\n","\n","Correlation is usually quantified by a correlation coefficient, which ranges from -1 to +1. The closer the coefficient is to either of these extremes, the stronger the correlation. A coefficient of 0 means there is no linear relationship between the two variables.\n","\n","Types of Correlation:\n","\n","Positive Correlation (+1): When one variable increases, the other variable also increases. For example, the height of a person and their weight might have a positive correlation.\n","\n","Negative Correlation (-1): When one variable increases, the other variable decreases. For example, the speed of a car and the time it takes to reach a destination might have a negative correlation.\n","\n","Zero or No Correlation (0): There's no predictable relationship between the two variables. For example, the correlation between shoe size and intelligence is likely to be near zero.\n","\n","What Does Negative Correlation Mean?\n","\n","Negative correlation means that as one variable increases, the other variable tends to decrease, and vice versa. The relationship is inverse or opposite. In terms of the correlation coefficient:\n","\n","A correlation of -1 indicates a perfect negative correlation, meaning that if one variable increases by a certain amount, the other decreases by a proportional amount.\n","\n","A correlation between 0 and -1 indicates varying degrees of negative correlation, where the strength of the inverse relationship is weaker as the coefficient moves closer to zero.\n","\n","Example of Negative Correlation:\n","\n","Example 1: Temperature and Heating Bill: As the outdoor temperature increases, the amount of money spent on heating (in cold climates) tends to decrease. This shows a negative correlation.\n","\n","Example 2: Exercise and Body Weight: If someone increases the amount of exercise they do, it could lead to a decrease in their body weight (assuming no other factors like diet are at play). This might show a negative correlation.\n","\n","Interpreting Negative Correlation:\n","\n","A strong negative correlation (closer to -1) means that one variable reliably decreases when the other increases.\n","\n","A weak negative correlation (closer to 0) means that the relationship is less consistent or weaker, and the changes in one variable don‚Äôt reliably cause changes in the other."],"metadata":{"id":"zeMoTEGIZYvy"}},{"cell_type":"markdown","source":["Machine Learning (ML) is a subset of artificial intelligence (AI) that enables computers to learn from data and make predictions or decisions without being explicitly programmed. In machine learning, algorithms analyze and identify patterns in data, and use these patterns to make predictions or decisions based on new, unseen data.\n","\n","The key idea behind machine learning is that systems can improve their performance over time as they are exposed to more data, effectively \"learning\" from the past rather than relying on hardcoded rules.\n","\n","Main Components of Machine Learning:\n","\n","Machine learning systems typically consist of several components that work together to process data and build models. These components can be broadly categorized into:\n","\n","Data:\n","\n","Input Data: The raw data that is used to train the model. This data can be in various forms, such as numbers, text, images, or sensor data.\n","\n","Feature Extraction: The process of transforming raw data into a set of features (attributes) that are more suitable for analysis. For example, turning raw pixel values into a set of features describing an image.\n","\n","Training Data vs. Test Data: Machine learning models are usually trained on one dataset (training data) and evaluated on another (test data) to avoid overfitting.\n","\n","Model:\n","\n","A model is the mathematical representation of the relationships or patterns in the data. It is created by training an algorithm on the training data. Examples of models include decision trees, neural networks, support vector machines (SVM), and linear regression.\n","The parameters of the model are the internal settings that the algorithm tunes during the learning process. For instance, in a neural network, the weights of the connections between neurons are parameters.\n","\n","Algorithm:\n","\n","An algorithm is a step-by-step procedure used to create the model. It dictates how the model learns from data. Different algorithms are suited to different types of learning tasks (e.g., classification, regression, clustering).\n","Common machine learning algorithms include:\n","Linear Regression (for predicting continuous values)\n","Decision Trees (for classification and regression)\n","Support Vector Machines (SVM) (for classification)\n","Neural Networks (for complex tasks like image and speech recognition)\n","K-Means (for clustering)\n","\n","Training Process:\n","\n","Training refers to the process of applying a machine learning algorithm to a dataset so that the model can learn the patterns in the data. During training, the algorithm adjusts the model‚Äôs parameters to minimize the error between its predictions and the actual outcomes in the training data.\n","This process often involves optimization techniques such as gradient descent, which iteratively adjusts the model‚Äôs parameters to minimize a loss function (a measure of how well the model's predictions match the actual data).\n","\n","Evaluation Metrics:\n","\n","After training, it‚Äôs important to evaluate the model‚Äôs performance using evaluation metrics. These metrics help determine how well the model generalizes to unseen data (i.e., its accuracy, precision, recall, F1 score, etc.).\n","Common evaluation metrics for classification tasks include accuracy, precision, recall, and the area under the curve (AUC). For regression tasks, common metrics include mean squared error (MSE) and R-squared.\n","\n","Testing and Validation:\n","\n","Test Data: A separate dataset not used during training, which is used to evaluate how well the model performs on unseen data.\n","\n","Cross-Validation: A technique used to assess how the model generalizes by splitting the dataset into several subsets (folds) and training/testing the model multiple times on different subsets.\n","\n","Overfitting & Underfitting: The model is evaluated to ensure it‚Äôs neither too complex (overfitting) nor too simple (underfitting). Overfitting occurs when the model learns the noise in the data, while underfitting occurs when the model fails to capture important patterns.\n","\n","Prediction:\n","\n","After training, the machine learning model can make predictions on new, unseen data. In supervised learning, these predictions are made based on patterns the model has learned from labeled training data (e.g., classifying new emails as spam or not spam). In unsupervised learning, predictions are made without labeled data, such as identifying clusters of similar data points."],"metadata":{"id":"ZiTnVh7wZzte"}},{"cell_type":"markdown","source":["In machine learning, the loss value (also called the loss function or cost function) is a critical metric that helps assess how well a model is performing. It quantifies the difference between the model's predictions and the actual outcomes (i.e., the true values). The loss value is a measure of error or discrepancy, and it plays a key role in guiding the model's learning process.\n","\n","How Loss Value Helps in Determining Whether a Model is Good:\n","\n","Quantifies Prediction Error:\n","\n","The loss function provides a numerical value that represents how far the model's predictions are from the actual values. A lower loss means the model's predictions are closer to the true values, indicating better performance.\n","For example, in regression tasks (predicting continuous values), common loss functions like Mean Squared Error (MSE) measure the average squared difference between predicted values and true values. A lower MSE means smaller differences between predictions and actual values.\n","\n","In classification tasks (e.g., binary or multi-class classification), loss functions like Cross-Entropy Loss measure the difference between the predicted probability distribution (for each class) and the true class label. A lower cross-entropy loss indicates better performance.\n","Optimization and Model Training:\n","\n","The primary goal during model training is to minimize the loss value by adjusting the model's parameters (e.g., weights in a neural network) through an optimization process (such as gradient descent).\n","\n","As the training progresses, the model should learn to predict better, which should result in a decreasing loss value. This shows that the model is gradually improving its performance.\n","\n","If the loss value decreases steadily over time, it suggests that the model is learning and improving. If the loss stops decreasing or increases, this might indicate problems like overfitting, underfitting, or issues with the learning process.\n","\n","Indicating Model Quality:\n","\n","The loss value itself does not directly tell you if the model is ‚Äúgood‚Äù or ‚Äúbad‚Äù but provides a way to monitor improvement during training.\n","Low loss generally means the model is making accurate predictions, while a high loss suggests poor performance. However, \"good\" or \"bad\" performance is also context-dependent, and loss alone doesn‚Äôt necessarily tell the full story. You often need to combine the loss value with other performance metrics to evaluate the model comprehensively.\n","\n","For example, in classification, you might look at accuracy, precision, recall, and F1 score in addition to the loss value.\n","In regression tasks, you might also examine metrics like R-squared or Mean Absolute Error (MAE).\n","\n","Loss Function Choice:\n","\n","The choice of loss function depends on the task and the type of model you are training. Different loss functions are suited to different problems, and the way the model improves can vary depending on the loss function used.\n","For regression tasks, loss functions like Mean Squared Error (MSE) or Mean Absolute Error (MAE) are common.\n","\n","For classification tasks, loss functions like Cross-Entropy Loss (for binary or multi-class classification) or Hinge Loss (for support vector machines) are used.\n","\n","In reinforcement learning, the loss can be related to the reward signal that the model receives during training.\n","Overfitting and Underfitting:\n","\n","The loss value can help detect issues like overfitting and underfitting:\n","Overfitting occurs when the model learns to perform very well on the training data but fails to generalize to new, unseen data. In this case, the training loss will be very low, but the test loss (evaluated on a separate test dataset) may be significantly higher.\n","\n","Underfitting occurs when the model fails to capture the underlying patterns in the data, resulting in both high training and high test loss.\n","Monitoring both training loss and validation/test loss can help determine whether the model is overfitting or underfitting."],"metadata":{"id":"CWN7yr60afhX"}},{"cell_type":"markdown","source":["In statistics and data analysis, variables are typically classified into two broad categories: continuous and categorical. These classifications depend on the type of data the variable represents and how it can take on different values.\n","\n","1. Continuous Variables:\n","\n","Continuous variables, also known as quantitative variables, are variables that can take on an infinite number of values within a given range. These values are measurable and can represent quantities that can be divided into smaller increments.\n","\n","Examples:\n","\n","Height (e.g., 5.5 feet, 5.52 feet, 5.528 feet, etc.)\n","\n","Weight (e.g., 70.1 kg, 70.12 kg, 70.123 kg, etc.)\n","\n","Temperature (e.g., 21.5¬∞C, 21.55¬∞C, 21.555¬∞C, etc.)\n","\n","Time (e.g., 3.4 hours, 3.45 hours, 3.452 hours, etc.)\n","\n","Characteristics:\n","\n","Continuous variables can take any real number value within a range.\n","They are typically measured, not counted.\n","They can have decimal or fractional values.\n","They often represent quantities that can change gradually.\n","\n","Mathematical Operations:\n","\n","We can perform arithmetic operations (addition, subtraction, multiplication, division) with continuous variables, and the results will still be meaningful.\n","\n","2. Categorical Variables:\n","\n","Categorical variables, also called qualitative variables, represent types or categories. These variables can take on a limited number of distinct values (categories), and these values do not have a meaningful order or magnitude.\n","\n","Examples:\n","\n","Nominal variables (no inherent order):\n","\n","Gender (e.g., Male, Female, Other)\n","\n","Eye color (e.g., Blue, Brown, Green)\n","\n","Marital status (e.g., Single, Married, Divorced)\n","\n","Ordinal variables (have a meaningful order, but the intervals between categories may not be equal):\n","\n","Education level (e.g., High school, Bachelor's, Master's, PhD)\n","\n","Likert scale responses (e.g., Strongly Disagree, Disagree, Neutral, Agree, Strongly Agree)\n","\n","Rating scales (e.g., 1 star, 2 stars, 3 stars, etc.)\n","\n","Characteristics:\n","\n","Categorical variables represent groups or categories.\n","The values are discrete and cannot be divided further.\n","In ordinal categorical variables, there is a meaningful order (but no consistent difference between categories), whereas in nominal variables, the categories have no natural order.\n","\n","Mathematical Operations:\n","\n","Arithmetic operations are not meaningful for categorical variables, although you can count occurrences or calculate the mode (most frequent category)."],"metadata":{"id":"czOVObN5bAYE"}},{"cell_type":"markdown","source":["Handling categorical variables is a crucial part of the machine learning pipeline, as most machine learning algorithms work with numerical data. Since categorical variables are often non-numeric (such as \"Gender\" or \"Country\"), they need to be converted into a suitable numerical form before they can be used in models. There are several techniques to do this, depending on the nature of the categorical data and the specific model being used.\n","\n","Here are the common techniques for handling categorical variables in machine learning:\n","\n","1. Label Encoding:\n","\n","Label encoding is a technique where each unique category in a categorical variable is assigned a unique integer label.\n","\n","How it works:\n","\n","Each category is mapped to a number (e.g., \"Red\" ‚Üí 0, \"Green\" ‚Üí 1, \"Blue\" ‚Üí 2).\n","This is typically used for ordinal categorical variables, where the order of the categories matters.\n","\n","Example:\n","For a variable \"Color\" with categories \"Red,\" \"Green,\" and \"Blue\":\n","\n","\"Red\" ‚Üí 0\n","\n","\"Green\" ‚Üí 1\n","\n","\"Blue\" ‚Üí 2\n","\n","Pros:\n","\n","Simple and fast.\n","\n","Useful for ordinal data (where the order of the categories matters).\n","\n","Cons:\n","\n","Doesn't handle nominal (non-ordered) data well because the model may interpret the numeric values as having some sort of relationship or hierarchy.\n","Not suitable for models like decision trees, where a variable's integer encoding might create artificial ordinal relationships.\n","When to use:\n","Ordinal categorical variables (e.g., \"Low\", \"Medium\", \"High\").\n","\n","2. One-Hot Encoding\n","\n","One-hot encoding creates binary (0/1) columns for each category in the categorical variable. For each category, a new column is created, and the value is marked as 1 if the category is present and 0 if it's absent.\n","\n","How it works:\n","If the variable has\n","ùëõ\n","n unique categories, one-hot encoding will create\n","ùëõ\n","n new columns.\n","For example, for the \"Color\" variable with categories \"Red,\" \"Green,\" and \"Blue,\" three new columns are created: one for \"Red,\" one for \"Green,\" and one for \"Blue.\" If the color is \"Red,\" the corresponding column would have a value of 1, and the other columns would have 0.\n","\n","Example:\n","\n","For the \"Color\" variable:\n","\n","Color\tRed\tGreen\tBlue\n","Red\t1\t0\t0\n","\n","Green\t0\t1\t0\n","\n","Blue\t0\t0\t1\n","\n","Pros:\n","\n","Works well with nominal (non-ordinal) categorical variables where no natural order exists.\n","Prevents any assumptions of ordinality or rank in the data.\n","\n","Cons:\n","\n","Increases the dimensionality of the dataset, which can lead to \"curse of dimensionality\" issues (i.e., a very large number of features if the categorical variable has many categories).\n","Not ideal for high-cardinality features (i.e., features with many unique categories, like zip codes or product IDs).\n","When to use:\n","Nominal categorical variables (e.g., \"City,\" \"Gender,\" \"Country\").\n","\n","3. Binary Encoding\n","\n","Binary encoding is a compromise between label encoding and one-hot encoding. It is suitable for categorical variables with a high cardinality (many unique categories). Binary encoding represents each category with a binary code and then splits the binary digits into separate columns.\n","\n","How it works:\n","\n","First, each category is assigned an integer.\n","Then, the integer is converted into its binary form.\n","Finally, each bit of the binary number is placed in a separate column.\n","\n","Example:\n","For the \"Color\" variable (with 4 categories: \"Red,\" \"Green,\" \"Blue,\" \"Yellow\"):\n","\n","Assign integers: Red ‚Üí 0, Green ‚Üí 1, Blue ‚Üí 2, Yellow ‚Üí 3.\n","\n","Convert to binary:\n","\n","Red = 00\n","\n","Green = 01\n","\n","Blue = 10\n","\n","Yellow = 11\n","\n","Separate the binary digits into columns:\n","\n","Color\tBit 1\tBit 2\n","\n","Red\t0\t0\n","\n","Green\t0\t1\n","\n","Blue\t1\t0\n","\n","Yellow\t1\t1\n","\n","Pros:\n","\n","Reduces dimensionality compared to one-hot encoding for high cardinality features.\n","Maintains a reasonable representation of categories without the curse of dimensionality.\n","\n","Cons:\n","The binary numbers can still impose a sense of ordinality that may confuse some models.\n","Not as simple as one-hot encoding and might require more understanding of how it works.\n","\n","When to use:\n","\n","When dealing with high-cardinality categorical variables (e.g., product IDs, user IDs, etc.).\n","\n","4. Target Encoding (Mean Encoding)\n","\n","Target encoding replaces categorical values with the mean of the target variable for each category. It is often used in regression or binary classification tasks, where the categorical variable is associated with a target variable (dependent variable).\n","\n","How it works:\n","\n","For each category in the categorical variable, calculate the mean of the target variable for all instances in that category.\n","Replace the original categorical value with this mean.\n","\n","Example:\n","For a binary classification problem with a target variable \"Churn\" (1 = churned, 0 = not churned) and a \"City\" feature:\n","\n","City\tChurn (Target)\tTarget Encoding (City)\n","NY\t1\t0.6\n","LA\t0\t0.3\n","NY\t0\t0.6\n","LA\t1\t0.3\n","\n","Pros:\n","Works well for high-cardinality categorical features.\n","Captures the relationship between the categorical variable and the target variable.\n","Cons:\n","Can lead to overfitting if not handled correctly, especially in small datasets (as it relies on the target variable's mean).\n","Care must be taken to prevent data leakage. One way to avoid this is to use cross-validation to compute the mean for each fold.\n","When to use:\n","In regression or binary classification tasks with high-cardinality features.\n","When there's a strong relationship between the categorical variable and the target variable.\n","\n","5. Frequency (Count) Encoding\n","Frequency encoding replaces each category with the frequency (or count) of that category in the dataset.\n","\n","How it works:\n","Calculate how many times each category appears in the dataset and use that number as the encoding.\n","Example:\n","For the \"City\" variable with values: [\"NY\", \"LA\", \"NY\", \"LA\", \"SF\"]:\n","\n","NY appears 2 times, LA appears 2 times, and SF appears 1 time. So, the encoding would be: | City | Frequency Encoding | |------|--------------------| | NY | 2 | | LA | 2 | | NY | 2 | | LA | 2 | | SF | 1 |\n","Pros:\n","Simple and works well when the frequency of categories is meaningful.\n","Can be less prone to overfitting than target encoding in some cases.\n","Cons:\n","Assumes that the frequency of a category is informative, which may not always be the case.\n","Can lose information, as the encoding is based only on the count, not the underlying distribution of the categories.\n","When to use:\n","When the frequency of categories is important and has predictive power.\n","\n","6. Embedding Methods (for Deep Learning)\n","\n","In deep learning, especially with neural networks, categorical variables can be handled using embedding layers. This is a learned representation of the categorical variable in a lower-dimensional continuous vector space.\n","\n","How it works:\n","\n","For each category, an embedding vector is learned during training. Each category is mapped to a continuous vector of real numbers, typically using an embedding layer in the neural network.\n","Pros:\n","Suitable for high-cardinality categorical variables.\n","Embeddings capture more nuanced relationships and interactions between categories.\n","Cons:\n","Requires a neural network setup and can be computationally expensive.\n","Not suitable for simple models or small datasets.\n","When to use:\n","When working with deep learning models (e.g., neural networks) and high-cardinality categorical features.\n"],"metadata":{"id":"ExluN8MVcHVl"}},{"cell_type":"markdown","source":["In machine learning, training and testing a dataset refer to two key stages in building and evaluating a model. These stages ensure that the model learns patterns from data and is then assessed for its ability to generalize to new, unseen data. Here‚Äôs a breakdown of what each stage involves:\n","\n","1. Training a Dataset:\n","\n","Training a model refers to the process where the machine learning algorithm learns from a set of data. During this stage, the model uses the training data to identify patterns, relationships, or structures in the data that can be used for prediction or classification.\n","\n","Key Concepts:\n","\n","Training Data: This is the dataset used to train the model. It contains both the features (input variables) and the target (output variable or label). The model \"learns\" by adjusting its internal parameters to minimize the error between its predictions and the actual outcomes in the training set.\n","\n","Model Fitting: The process of training the model is often referred to as \"fitting\" the model to the training data. The model‚Äôs parameters (weights, coefficients, etc.) are adjusted during this phase to improve its accuracy on the training set.\n","\n","Objective: The goal during training is for the model to minimize the loss function (which quantifies the difference between predicted and actual values). For example, in regression problems, the model might minimize Mean Squared Error (MSE), while in classification problems, it could minimize cross-entropy loss.\n","\n","Example:\n","Imagine you're building a model to predict house prices based on features like square footage, location, and number of bedrooms. You would use a dataset that contains information about houses (features) and their corresponding sale prices (target) to train the model.\n","\n","During training, the model would adjust its internal parameters to learn how the features (e.g., square footage) relate to the target (sale price).\n","\n","2. Testing a Dataset\n","\n","Testing a model involves evaluating how well it performs on data that it has not seen before. This is done using the test data, which is separate from the training data. The purpose of testing is to assess how well the model can generalize to new, unseen examples ‚Äî this is critical because the ultimate goal of a machine learning model is to make accurate predictions on real-world data, not just on the data it was trained on.\n","\n","Key Concepts:\n","\n","Test Data: This is a separate portion of the dataset that is used to evaluate the model‚Äôs performance. The model does not see the test data during training, which means it has no opportunity to \"memorize\" the answers. The test data should be representative of the real-world data the model will encounter in production.\n","\n","Model Evaluation: After training, the model's performance is evaluated on the test data using various evaluation metrics, such as:\n","\n","Accuracy (for classification)\n","Precision, Recall, F1-Score (for classification)\n","Mean Absolute Error (MAE) or Mean Squared Error (MSE) (for regression)\n","Confusion Matrix (for classification problems)\n","Objective: The goal during testing is to assess how well the model has generalized to unseen data. A model that performs well on the training data but poorly on the test data might be overfitting.\n","\n","Example:\n","Using the house price example above, after training the model on the training data, you would then test the model on a separate set of data ‚Äî houses the model has never seen before. The model's predictions (e.g., predicted house prices) would be compared to the actual prices in the test set, and performance metrics like Mean Squared Error (MSE) would be calculated to determine how well the model generalizes."],"metadata":{"id":"TBbFPms6dM9-"}},{"cell_type":"markdown","source":["sklearn.preprocessing is a module in scikit-learn (often abbreviated as sklearn), a popular Python library for machine learning. The preprocessing module provides a variety of functions and classes for preprocessing and transforming data before applying machine learning algorithms. This step is crucial because the performance of many machine learning models depends heavily on the data's format, scaling, and encoding.\n","\n","Key Functions and Classes in sklearn.preprocessing\n","\n","Here‚Äôs a breakdown of some of the most commonly used functions and classes in sklearn.preprocessing:\n","\n","1. StandardScaler\n","\n","StandardScaler standardizes features by removing the mean and scaling to unit variance (i.e., normalizes the data). This is useful when the features have different units or scales, as many algorithms (e.g., linear regression, k-nearest neighbors) perform better when the features are on a similar scale.\n","\n","Usage:\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","\n","X_scaled = scaler.fit_transform(X)\n","\n","Details:\n","\n","fit: Calculates the mean and standard deviation of the features in the data.\n","transform: Applies the standardization (scaling) using the calculated mean and standard deviation.\n","fit_transform: Combines both the fit and transform steps.\n","\n","2. MinMaxScaler\n","\n","MinMaxScaler rescales the features to a specific range, usually between 0 and 1. This is often used when you want to ensure that each feature has the same scale, but unlike standardization, it doesn‚Äôt assume normal distribution.\n","\n","Usage:\n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","scaler = MinMaxScaler()\n","\n","X_scaled = scaler.fit_transform(X)\n","\n","Details:\n","\n","The formula for scaling is:\n","ùëã\n","scaled\n","=\n","ùëã\n","‚àí\n","min\n","(\n","ùëã\n",")\n","max\n","(\n","ùëã\n",")\n","‚àí\n","min\n","(\n","ùëã\n",")\n","X\n","scaled\n","‚Äã\n"," =\n","max(X)‚àímin(X)\n","X‚àímin(X)\n","‚Äã\n","\n","Works well when you know the features are within a specific range and you want to normalize them to fit within that range (like between 0 and 1).\n","\n","3. RobustScaler\n","RobustScaler scales the data using the median and interquartile range (IQR). It is more robust to outliers than StandardScaler because it uses the median (which is less sensitive to extreme values) instead of the mean.\n","\n","Usage:\n","\n","\n","from sklearn.preprocessing import RobustScaler\n","\n","scaler = RobustScaler()\n","\n","X_scaled = scaler.fit_transform(X)\n","\n","Details:\n","\n","The formula for scaling is:\n","ùëã\n","scaled\n","=\n","ùëã\n","‚àí\n","median\n","(\n","ùëã\n",")\n","IQR\n","(\n","ùëã\n",")\n","X\n","scaled\n","‚Äã\n"," =\n","IQR(X)\n","X‚àímedian(X)\n","‚Äã\n","\n","Particularly useful when the dataset contains outliers.\n","\n","4. Normalizer\n","\n","Normalizer scales individual samples (i.e., rows of data) to have unit norm (length). It is typically used in situations where you want to scale the vectors of each individual sample (e.g., in text classification or document classification using TF-IDF or word embeddings).\n","\n","Usage:\n","\n","from sklearn.preprocessing import Normalizer\n","\n","normalizer = Normalizer()\n","\n","X_normalized = normalizer.fit_transform(X)\n","\n","Details:\n","\n","Each sample is scaled to have a Euclidean norm (length) of 1.\n","Useful when each sample (e.g., each document or each observation) represents a feature vector and needs to be normalized independently.\n","\n","5. OneHotEncoder\n","\n","OneHotEncoder is used to convert categorical variables (which are non-numeric) into a binary matrix. Each unique category in a feature gets a new column, and the presence of that category in the row is represented by a 1 (otherwise 0). This is widely used for handling categorical features.\n","\n","Usage:\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","encoder = OneHotEncoder()\n","\n","X_encoded = encoder.fit_transform(X)\n","\n","Details:\n","\n","Converts categorical values into a one-hot encoded format.\n","By default, it returns a sparse matrix, but you can use toarray() to convert it into a dense array.\n","\n","Example:\n","For a feature Color with categories [\"Red\", \"Green\", \"Blue\"], the output would be:\n","\n","Color\tRed\tGreen\tBlue\n","\n","Red\t1\t0\t0\n","\n","Green\t0\t1\t0\n","\n","Blue\t0\t0\t1\n","\n","6. LabelEncoder\n","\n","LabelEncoder is used to convert categorical labels (target variable) into numeric labels. It is often used when you have a target variable with categorical values (e.g., ‚ÄúYes‚Äù and ‚ÄúNo‚Äù or ‚ÄúCat‚Äù and ‚ÄúDog‚Äù) and you want to convert these categories into numbers for a classification problem.\n","\n","Usage:\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","encoder = LabelEncoder()\n","\n","y_encoded = encoder.fit_transform(y)\n","\n","Details:\n","\n","Converts categorical labels (e.g., [\"cat\", \"dog\", \"fish\"]) into numerical labels (e.g., [0, 1, 2]).\n","Useful for classification tasks where the target variable is categorical.\n","\n","7. Binarizer\n","\n","Binarizer is used to binarize (convert to 0 or 1) the data based on a given threshold. This is useful when you want to convert continuous data into binary data based on a threshold value.\n","\n","Usage:\n","\n","from sklearn.preprocessing import Binarizer\n","\n","scaler = Binarizer(threshold=0.0)\n","\n","X_binarized = scaler.fit_transform(X)\n","\n","Details:\n","\n","All values above the threshold are set to 1, and all values below the threshold are set to 0.\n","\n","Commonly used for feature engineering or when transforming data into a binary format.\n","\n","8. PolynomialFeatures\n","\n","PolynomialFeatures is used to generate polynomial and interaction features. This is particularly useful when you want to introduce higher-order features (like quadratic, cubic, or interaction terms) into a model to capture non-linear relationships between the input features.\n","\n","Usage:\n","\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","poly = PolynomialFeatures(degree=2)  # Create polynomial features of degree 2\n","\n","X_poly = poly.fit_transform(X)"],"metadata":{"id":"YoprDwhSdrt5"}},{"cell_type":"markdown","source":["A test set in machine learning is a portion of the dataset that is used to evaluate the performance of a model after it has been trained. The test set serves as an independent dataset that the model has never seen during training, allowing you to assess how well the model is likely to perform on unseen data in real-world scenarios.\n","\n","Key Characteristics of a Test Set:\n","\n","Independent Data:\n","\n","The test set should consist of data that was not used during the model's training process. This ensures that the evaluation results provide a true measure of how the model will perform on new, real-world data.\n","\n","Evaluation Metric:\n","\n","After the model is trained on the training data, it is evaluated on the test set to determine how well it generalizes. Common evaluation metrics for regression tasks include Mean Squared Error (MSE), Mean Absolute Error (MAE), or R-squared, while for classification tasks, you might use accuracy, precision, recall, F1-score, or ROC-AUC.\n","\n","Final Model Assessment:\n","\n","The test set provides an unbiased estimate of the model‚Äôs real-world performance and is used to judge its generalization ability (i.e., how well the model performs on new data).\n","\n","Not Used in Training or Tuning:\n","\n","The test set should not influence the model during training or hyperparameter tuning. If you use the test set for any decision-making (like selecting hyperparameters or deciding on the final model), it would no longer be considered a true test set, leading to data leakage or overfitting.\n","\n","Why is a Test Set Important?\n","\n","Prevents Overfitting: By using a separate test set, you ensure that the model has not memorized the training data, which could lead to overfitting. Overfitting occurs when a model performs well on the training data but poorly on new, unseen data.\n","\n","Estimates Real-World Performance: The test set helps provide an estimate of how the model will behave when deployed to make predictions on new, real-world data.\n","\n","Ensures Fair Evaluation: It ensures that the evaluation is based on data the model has not been exposed to, giving a fair picture of its performance.\n","\n","How is the Dataset Split into Training, Validation, and Test Sets?\n","\n","Typically, the dataset is split into three main subsets:\n","\n","Training Set:\n","\n","Used to train the model. The model learns patterns from this data.\n","Common size: 60-80% of the entire dataset.\n","\n","Validation Set (optional but common in many machine learning workflows):\n","\n","Used during training to fine-tune hyperparameters or select between different models.\n","It helps to avoid overfitting the model to the training data by providing a performance metric on data the model has not seen before.\n","Common size: 10-20% of the dataset.\n","\n","Test Set:\n","\n","Used to evaluate the model after it has been trained and validated. This set is only used for final evaluation.\n","Common size: 10-20% of the dataset.\n","How to Split the Dataset?\n","\n","Random Split:\n","\n","The data is randomly divided into the training and test sets. Typically, you might use a 70%-30% or 80%-20% split, with 70-80% for training and 20-30% for testing.\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n","random_state=42)\n","\n","Stratified Split (for classification problems):\n","\n","In situations where the target variable is imbalanced (e.g., there are significantly more instances of one class than another), a stratified split ensures that the distribution of the target variable is similar in both the training and test sets.\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","\n","K-Fold Cross-Validation (for more robust testing):\n","\n","Instead of using a fixed test set, cross-validation divides the dataset into k subsets (or folds) and evaluates the model k times, each time using a different fold as the test set and the remaining data as the training set. This provides a more robust estimate of model performance by using multiple splits.\n","\n","from sklearn.model_selection import cross_val_score\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","model = RandomForestClassifier()\n","\n","scores = cross_val_score(model, X, y, cv=5)  # 5-fold cross-validation\n"],"metadata":{"id":"ntBjTGWlfO6u"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Sample feature data and target labels\n","X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]  # Example feature data\n","y = [0, 1, 0, 1, 0]  # Example target labels\n","\n","# Split into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(\"Training Features:\\n\", X_train)\n","print(\"Test Features:\\n\", X_test)\n","print(\"Training Labels:\\n\", y_train)\n","print(\"Test Labels:\\n\", y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDjSdWCDgiqz","executionInfo":{"status":"ok","timestamp":1734789500056,"user_tz":-330,"elapsed":3592,"user":{"displayName":"Varun Garg","userId":"11663094782443047067"}},"outputId":"3e991210-c518-4e80-c1c4-19d2bb691a66"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Features:\n"," [[9, 10], [5, 6], [1, 2], [7, 8]]\n","Test Features:\n"," [[3, 4]]\n","Training Labels:\n"," [0, 0, 0, 1]\n","Test Labels:\n"," [1]\n"]}]},{"cell_type":"markdown","source":["How to Approach a Machine Learning Problem\n","\n","The typical process of approaching a machine learning problem involves several key steps. Here's a structured approach:\n","\n","1. Problem Definition\n","\n","Before diving into any machine learning tasks, it's essential to define the problem clearly. Identify:\n","\n","\n","The type of problem: Is it a classification (predicting categories) or regression (predicting continuous values) problem?\n","The objective: What exactly do you want to predict or classify?\n","The data: Do you have labeled data (supervised learning) or unlabeled data (unsupervised learning)?\n","2. Data Collection\n","\n","Gather the data that is relevant to your problem. The quality of your data will significantly influence the performance of the model. If you don't have your own dataset, you may need to:\n","\n","Collect it from public datasets (e.g., Kaggle, UCI Machine Learning Repository).\n","Use APIs (e.g., Twitter API, Google Cloud Storage) to pull data from external sources.\n","Scrape data from websites.\n","\n","3. Data Exploration and Preprocessing\n","\n","Explore the Data: Look at the basic structure of the data:\n","\n","Descriptive statistics: Mean, median, standard deviation, correlations, etc.\n","Visualizations: Histograms, box plots, scatter plots to understand distributions and relationships.\n","\n","Preprocess the Data: This step includes several sub-steps:\n","\n","Handling Missing Values: Remove or impute missing data (using the mean, median, or more sophisticated techniques).\n","\n","Encoding Categorical Data: Convert non-numeric categorical features into numeric values (e.g., One-Hot Encoding or Label Encoding).\n","\n","Feature Scaling: Normalize or standardize features to ensure they are on a similar scale (e.g., using StandardScaler or MinMaxScaler).\n","\n","Feature Selection: Select relevant features, remove irrelevant or redundant ones, and engineer new features that might improve model performance.\n","\n","4. Data Splitting\n","As discussed earlier, split the data into training and testing sets (and possibly a validation set). Common ratios are:\n","\n","Training set: 70-80% of the data\n","Test set: 20-30% of the data\n","Optionally, a validation set for hyperparameter tuning.\n","\n","5. Model Selection\n","\n","Choose an appropriate model based on the type of problem and the characteristics of the data:\n","\n","For classification problems, you might start with models like Logistic Regression, Decision Trees, Random Forests, SVM, or k-Nearest Neighbors.\n","For regression problems, models like Linear Regression, Ridge/Lasso Regression, Random Forest Regression, or SVR might be good starting points.\n","You might also experiment with more complex models like Neural Networks or Gradient Boosting (e.g., XGBoost, LightGBM).\n","\n","6. Model Training\n","\n","Train the selected model on the training data. During training:\n","\n","The model learns patterns from the training features (X_train) and maps them to the target labels (y_train).\n","Depending on the algorithm, you may tune the model's hyperparameters (e.g., learning rate, number of trees, regularization strength"],"metadata":{"id":"J6ce4MEvgyEC"}},{"cell_type":"markdown","source":["Performing Exploratory Data Analysis (EDA) before fitting a machine learning model is a crucial step in the data science workflow. EDA allows you to better understand your dataset, uncover important patterns, identify potential issues, and make informed decisions about how to proceed with modeling. Here's why EDA is so important:\n","\n","1. Understand Data Distribution\n","\n","Purpose: EDA helps you understand the underlying structure and distribution of your data, which is essential for selecting the right machine learning model and preprocessing techniques.\n","\n","How: You can explore the statistical properties of each feature, such as the mean, median, standard deviation, range, and quartiles, and visualize distributions through histograms, box plots, or density plots.\n","\n","Benefit: Knowing whether your data is skewed, has outliers, or follows a normal distribution will help you choose the appropriate model and transformations (e.g., logarithmic scaling for skewed data or standardization for normally distributed data).\n","Example:\n","\n","If a feature is normally distributed, models like linear regression or SVMs that assume normality may perform well.\n","If a feature is highly skewed, it may benefit from a log transformation.\n","\n","2. Detect Missing or Inconsistent Data\n","\n","Purpose: Real-world data is often incomplete, noisy, or inconsistent. EDA allows you to identify and handle missing, duplicate, or incorrectly formatted data.\n","\n","How: You can check for missing values, analyze the pattern of missingness, and assess whether data imputation or removal is necessary.\n","\n","Benefit: Properly handling missing data ensures that you don't introduce bias or errors in your model training.\n","\n","Example:\n","\n","If 90% of a column has missing values, it might be best to drop that feature, as it won't provide much information to the model.\n","For columns with random missing values, you may choose to impute them using the mean, median, or more sophisticated imputation techniques.\n","\n","3. Identify Outliers and Anomalies\n","\n","Purpose: Outliers and anomalies can have a significant impact on certain models, especially those based on distance (e.g., KNN, SVM) or regression (e.g., Linear Regression).\n","\n","How: Visualizations like box plots, scatter plots, or using Z-scores can help you identify data points that fall far outside the typical range.\n","\n","Benefit: Detecting outliers early helps you decide whether to remove, transform, or keep them in the dataset. For example, outliers might indicate data entry errors, or they could represent rare but important events (e.g., fraud detection).\n","Example:\n","\n","In a dataset where you're predicting house prices, an outlier with a price of $100 million may be an error, but it could also represent a unique property, depending on the context.\n","\n","You may choose to cap outliers to a maximum value or use robust models like Random Forests or Gradient Boosting that are less sensitive to outliers.\n","\n","4. Check for Correlation Among Features\n","\n","Purpose: High correlation between features (multicollinearity) can lead to redundant information, making some features irrelevant or problematic for certain models, especially linear models.\n","\n","How: Use correlation matrices or scatter plots to assess the relationships between features.\n","\n","Benefit: By identifying highly correlated features, you can reduce dimensionality (e.g., by removing redundant features or using dimensionality reduction techniques like PCA).\n","\n","Example:\n","\n","\n","If two features, such as Height and Weight, are highly correlated, you may decide to remove one of them to avoid multicollinearity, especially when using linear regression.\n","\n","5. Assess the Target Variable (y)\n","\n","Purpose: Understanding the target variable is essential for selecting the right model and evaluation metrics.\n","\n","How: Analyze the distribution of the target variable, check for class imbalance (in classification problems), and look for any trends or patterns.\n","\n","Benefit: This insight will help you choose the correct approach. For instance, if the target is imbalanced (e.g., 90% of one class and 10% of another), you may need to apply techniques like oversampling, undersampling, or use algorithms that handle imbalanced data well.\n","\n","Example:\n","\n","In a classification task, if 90% of the data points belong to class A and only 10% to class B, using accuracy as an evaluation metric may be misleading. You would need to focus on metrics like precision, recall, F1-score, or ROC-AUC.\n","\n","6. Identify the Need for Feature Engineering\n","\n","Purpose: Feature engineering plays a key role in improving model performance. EDA helps you understand how to transform, combine, or create new features that could better represent the underlying data.\n","\n","How: Explore relationships between features (e.g., creating interaction terms) or apply domain knowledge to create new features.\n","\n","Benefit: Feature engineering can dramatically improve model performance by making it easier for the model to learn relevant patterns.\n","\n","Example:\n","\n","In a dataset of customer information, combining features like Age and Income might yield a new feature called Income per Year of Age, which could be a better indicator of spending behavior.\n","\n","7. Choose the Right Preprocessing Techniques\n","\n","Purpose: The results from EDA can guide your decisions on preprocessing techniques, such as scaling, encoding, or normalization.\n","\n","How: EDA tells you whether scaling is needed (e.g., using StandardScaler for normally distributed features), whether you need to encode categorical variables, and which encoding method to use (e.g., one-hot encoding vs. label encoding).\n","\n","Benefit: Preprocessing the data appropriately ensures that the machine learning algorithm can learn effectively.\n","\n","Example:\n","\n","If a feature is categorical (e.g., \"Country\" with values [\"India\", \"Canada\", \"Mexico\"]), you would need to apply One-Hot Encoding.\n","\n","If your features are on different scales (e.g., \"Age\" vs. \"Income\"), you may want to standardize the features so they are on the same scale.\n","\n","8. Test Assumptions of Models\n","\n","Purpose: Many machine learning models make certain assumptions about the data (e.g., linearity, normality, homoscedasticity). EDA allows you to check whether your data meets these assumptions.\n","\n","How: Visualizing relationships between features and checking residuals from a model can help test assumptions.\n","\n","Benefit: Knowing whether the assumptions hold true can help you select the best model or indicate whether you need to transform the data or use a different modeling approach.\n","\n","Example:\n","\n","Linear regression assumes a linear relationship between features and the target. If this assumption is violated, you might need to transform the data or choose a different model like Decision Trees or Random Forests.\n","\n","9. Identify Data Imbalances and Sampling Issues\n","\n","Purpose: EDA helps you identify imbalances in the data, such as class imbalances in classification tasks or skewed distributions in regression tasks.\n","\n","How: Visualizing the target distribution (for classification) or the feature distributions can highlight potential issues.\n","\n","Benefit: By detecting imbalances early, you can apply techniques like resampling, class weighting, or use algorithms that are less sensitive to class imbalance (e.g., Random Forests).\n","\n","Example:\n","\n","If you are working on a binary classification task where 95% of the data belongs to class 0 and only 5% to class 1, the model will likely be biased toward predicting the majority class. You might need to apply oversampling to the minority class or use metrics like Precision-Recall AUC.\n"],"metadata":{"id":"qCZI_Hh3hcsA"}},{"cell_type":"markdown","source":["Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. In simpler terms, it tells us whether and how strongly two variables are related to each other.\n","\n","\n","If two variables tend to increase or decrease together, they are said to have a positive correlation.\n","\n","If one variable increases while the other decreases, they have a negative correlation.\n","\n","If there is no predictable relationship between the variables, they are said to have zero or no correlation.\n","\n","Types of Correlation\n","\n","Positive Correlation:\n","\n","When one variable increases, the other also increases (or when one decreases, the other decreases).\n","\n","Example: The relationship between height and weight. Generally, as height increases, weight also increases.\n","\n","Mathematical representation: If X and Y are two variables, and as X increases, Y also increases, the correlation will be positive. For example, a correlation coefficient of +0.8 suggests a strong positive relationship.\n","\n","Graphically: The points on a scatter plot form an upward-sloping line.\n","\n","Negative Correlation:\n","\n","When one variable increases, the other decreases (or vice versa).\n","\n","Example: The relationship between the number of hours spent watching TV and exam scores. As TV watching increases, exam scores tend to decrease.\n","\n","Mathematical representation: If X increases and Y decreases, the correlation will be negative. For example, a correlation coefficient of -0.7 suggests a strong negative relationship.\n","\n","Graphically: The points on a scatter plot form a downward-sloping line.\n","\n","Zero or No Correlation:\n","\n","There is no predictable relationship between the two variables. Changes in one variable don't affect the other.\n","\n","Example: The relationship between shoe size and intelligence. There's no reason to believe one affects the other.\n","\n","Mathematical representation: A correlation coefficient of 0 suggests no relationship between X and Y.\n","\n","Graphically: The points on a scatter plot appear scattered with no clear trend.\n","\n","Correlation Coefficient\n","\n","The correlation coefficient (often represented by r) is a numerical value that quantifies the degree of correlation between two variables. It ranges from -1 to +1:\n","\n","+1: Perfect positive correlation (both variables move in the same direction in perfect harmony).\n","\n","0: No correlation (the variables have no relationship).\n","\n","-1: Perfect negative correlation (one variable increases as the other decreases in perfect harmony).\n","\n","Formula for the Pearson Correlation Coefficient:\n","\n","For two variables\n","\n","ùëã\n","X and\n","ùëå\n","Y, the Pearson correlation coefficient\n","ùëü\n","r is calculated as:\n","\n","ùëü\n","=\n","‚àë\n","(\n","ùëã\n","ùëñ\n","‚àí\n","ùëã\n","‚Äæ\n",")\n","(\n","ùëå\n","ùëñ\n","‚àí\n","ùëå\n","‚Äæ\n",")\n","‚àë\n","(\n","ùëã\n","ùëñ\n","‚àí\n","ùëã\n","‚Äæ\n",")\n","2\n","‚àë\n","(\n","ùëå\n","ùëñ\n","‚àí\n","ùëå\n","‚Äæ\n",")\n","2\n","r=\n","‚àë(X\n","i\n","‚Äã\n"," ‚àí\n","X\n"," )\n","2\n"," ‚àë(Y\n","i\n","‚Äã\n"," ‚àí\n","Y\n"," )\n","2\n","\n","‚Äã\n","\n","‚àë(X\n","i\n","‚Äã\n"," ‚àí\n","X\n"," )(Y\n","i\n","‚Äã\n"," ‚àí\n","Y\n"," )\n","‚Äã\n","\n","Where:\n","\n","ùëã\n","ùëñ\n","X\n","i\n","‚Äã\n","  and\n","ùëå\n","ùëñ\n","Y\n","i\n","‚Äã\n","  are individual data points.\n","ùëã\n","‚Äæ\n","X\n","  and\n","ùëå\n","‚Äæ\n","Y\n","  are the means of\n","ùëã\n","X and\n","ùëå\n","Y, respectively.\n","\n","Interpreting the Correlation Coefficient\n","\n","0.0 to 0.3: Weak positive correlation\n","\n","0.3 to 0.7: Moderate positive correlation\n","\n","0.7 to 1.0: Strong positive correlation\n","\n","-0.3 to 0.0: Weak negative correlation\n","\n","-0.7 to -0.3: Moderate negative correlation\n","\n","-1.0 to -0.7: Strong negative correlation"],"metadata":{"id":"8hOxGT3djHen"}},{"cell_type":"markdown","source":["Negative correlation refers to a relationship between two variables where, as one variable increases, the other variable decreases, or vice versa. In simpler terms, when one variable goes up, the other tends to go down.\n","\n","Key Features of Negative Correlation:\n","\n","Inverse Relationship: Negative correlation represents an inverse or opposite relationship between two variables.\n","\n","Correlation Coefficient: The Pearson correlation coefficient for a negative correlation will be between -1 and 0:\n","\n","-1: A perfect negative correlation (when one variable increases, the other decreases in exact proportion).\n","\n","0: No correlation (the variables are unrelated).\n","\n","Between -1 and 0: A partial or moderate negative correlation (as one variable increases, the other generally decreases, but not perfectly).\n","\n","Example of Negative Correlation:\n","\n","Example 1: Height vs. Speed: In a group of athletes, as the height of a runner increases, their speed might decrease (up to a point), showing a negative correlation between height and speed.\n","\n","Example 2: Temperature vs. Winter Coat Sales: As the temperature increases (becomes warmer), the sales of winter coats tend to decrease. In this case, temperature and coat sales have a negative correlation.\n","\n","Example 3: Time Spent on TV vs. Exam Scores: As the number of hours spent watching TV increases, exam scores tend to decrease. This might indicate that excessive TV watching could negatively affect study time, thus lowering exam performance.\n","\n","Mathematical Interpretation:\n","\n","The correlation coefficient\n","ùëü\n","r quantifies the degree of the negative relationship:\n","\n","If\n","ùëü\n","r is close to -1, the negative relationship is strong, meaning the variables are inversely proportional to each other (e.g., as one variable increases, the other decreases in a predictable manner).\n","If\n","ùëü\n","r is closer to 0, the negative relationship is weaker, meaning there might be some negative trend, but the relationship isn't as clear or strong.\n","Visualizing Negative Correlation:\n","In a scatter plot, you will see points that tend to form a downward slope:\n","\n","If you plot x on the x-axis and y on the y-axis, as x increases, y decreases, and the points on the plot will trend from the upper-left corner to the lower-right corner."],"metadata":{"id":"BRmc4vS_k9zv"}},{"cell_type":"code","source":["# Using Pandas .corr() Method\n","import pandas as pd\n","\n","# Sample data\n","data = {\n","    'Height': [5.1, 5.9, 6.0, 5.5, 5.7],\n","    'Weight': [55, 75, 80, 70, 72],\n","    'Age': [23, 25, 22, 24, 27]\n","}\n","\n","# Creating DataFrame\n","df = pd.DataFrame(data)\n","\n","# Calculate correlation matrix\n","correlation_matrix = df.corr()\n","\n","# Display correlation matrix\n","print(correlation_matrix)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dh-4meBHllBx","executionInfo":{"status":"ok","timestamp":1734790825791,"user_tz":-330,"elapsed":434,"user":{"displayName":"Varun Garg","userId":"11663094782443047067"}},"outputId":"24a0902d-ad2b-4f5f-9275-84018132f8f9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["          Height    Weight       Age\n","Height  1.000000  0.975638  0.094451\n","Weight  0.975638  1.000000  0.077455\n","Age     0.094451  0.077455  1.000000\n"]}]},{"cell_type":"code","source":["# Using Seaborn to Visualize the Correlation Matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Sample data (same as above)\n","df = pd.DataFrame({\n","    'Height': [5.1, 5.9, 6.0, 5.5, 5.7],\n","    'Weight': [55, 75, 80, 70, 72],\n","    'Age': [23, 25, 22, 24, 27]\n","})\n","\n","# Create the correlation matrix\n","corr_matrix = df.corr()\n","\n","# Create a heatmap of the correlation matrix\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n","\n","# Display the plot\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":435},"id":"-Es6grjTl0Ki","executionInfo":{"status":"ok","timestamp":1734790875633,"user_tz":-330,"elapsed":2101,"user":{"displayName":"Varun Garg","userId":"11663094782443047067"}},"outputId":"f3b5cd06-bb41-408b-8c48-800dd13ea74f"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["Causation refers to the relationship where one variable directly influences or causes a change in another variable. In other words, causation means that a change in one variable produces a change in the other variable.\n","\n","Causal Relationship: A causal relationship exists when one event (the cause) directly results in another event (the effect).\n","\n","Causal Link: The cause precedes the effect in time, and the effect would not occur without the cause.\n","\n","Key Characteristics of Causation:\n","\n","Direction: The cause leads to the effect. It is a directional relationship, meaning the cause happens first and leads to a change in the effect.\n","\n","Mechanism: There is often a plausible mechanism explaining how the cause leads to the effect.\n","\n","Temporal Sequence: The cause must occur before the effect.\n","Exclusion of Confounding Variables: In a true causal relationship, other variables (confounders) must be ruled out as potential causes.\n","\n","Difference Between Correlation and Causation\n","\n","While correlation and causation are related concepts, they are fundamentally different:\n","\n","Correlation refers to a statistical relationship between two variables, indicating that they tend to vary together. However, correlation alone does not imply that one variable causes the other to change.\n","\n","Causation indicates that one variable directly influences the other, meaning that the change in one variable leads to a change in the other.\n","\n","Example of Correlation vs. Causation\n","\n","1. Correlation Example (False Relationship):\n","\n","Ice Cream Sales and Drowning Deaths:\n","\n","Observation: Data might show that as ice cream sales increase, the number of drowning deaths also increases.\n","\n","Correlation: The correlation between ice cream sales and drowning deaths is likely positive, meaning that they seem to rise together.\n","\n","However, this does not mean that eating ice cream causes drowning deaths. The true cause is likely a third variable ‚Äî warm weather. During hot weather, more people buy ice cream and go swimming, which increases the risk of drowning.\n","In this case, the relationship between ice cream sales and drowning deaths is correlation, not causation, because a third factor (temperature) is driving both.\n","\n","2. Causation Example (True Cause-Effect Relationship):\n","\n","Smoking and Lung Cancer:\n","\n","Observation: A study might show that people who smoke are at a much higher risk of developing lung cancer.\n","\n","Causation: Extensive research, including controlled experiments and statistical analyses, has shown that smoking directly causes lung cancer. The mechanism is understood ‚Äî the carcinogens in cigarette smoke damage lung cells, which can lead to cancer.\n","\n","In this case, the relationship between smoking and lung cancer is causal, because smoking directly leads to the development of lung cancer."],"metadata":{"id":"OiTUcPGXmMi5"}},{"cell_type":"markdown","source":["An optimizer is a key component of machine learning algorithms, particularly in training neural networks and deep learning models. Its main goal is to adjust the model's parameters (such as weights and biases) in order to minimize the loss function (or cost function). The optimizer does this through a process of iterative updates based on the gradients computed during backpropagation.\n","\n","Optimizers are critical for the training of machine learning models because they help find the best set of parameters that result in the most accurate model. They control how the weights are updated to minimize the loss function (the difference between the model's predictions and actual values).\n","\n","How Do Optimizers Work?\n","\n","Optimizers use the concept of gradient descent, which is an algorithm that updates the model's parameters in the opposite direction of the gradient of the loss function with respect to the parameters. The size of each step in this direction is determined by the learning rate.\n","\n","Key Concepts:\n","\n","Loss Function: A function that calculates the error or discrepancy between the model's prediction and the actual target values.\n","\n","Gradient: The derivative of the loss function with respect to the model parameters. It tells us the direction and magnitude of change needed to minimize the loss.\n","\n","Learning Rate: A hyperparameter that controls how big each step in the optimization process is.\n"],"metadata":{"id":"x69XE9TQm0j_"}},{"cell_type":"markdown","source":["In Scikit-learn (commonly referred to as sklearn), the linear_model module provides a set of algorithms for linear regression and classification. These algorithms are based on linear models, where the output is predicted as a linear combination of the input features. Linear models are foundational in machine learning and statistical analysis.\n","\n","The sklearn.linear_model module includes tools for:\n","\n","Linear regression (for regression tasks).\n","\n","Logistic regression (for binary and multiclass classification tasks).\n","\n","Ridge and Lasso regression (for regularized regression).\n","\n","ElasticNet (a combination of Ridge and Lasso).\n","\n","Linear Support Vector Machines (SVM).\n","\n","Poisson regression, Huber regression, and others.\n","These algorithms are widely used because of their simplicity, interpretability, and efficiency, especially when working with structured or tabular data.\n","\n","Key Components of sklearn.linear_model\n","\n","Here are the major classes and models available in the sklearn.linear_model module:\n","\n","1. Linear Regression (Ordinary Least Squares)\n","Description: The most basic linear model, used for predicting a continuous target variable. It assumes that the relationship between the input features and the target is linear.\n","\n","Class: sklearn.linear_model.LinearRegression\n","\n","Example:\n","\n","from sklearn.linear_model import LinearRegression\n","import numpy as np\n","\n","# Sample data\n","X = np.array([[1], [2], [3], [4], [5]])  # Features\n","y = np.array([1, 2, 3, 4, 5])  # Target\n","\n","# Create and train the model\n","model = LinearRegression()\n","model.fit(X, y)\n","\n","# Make predictions\n","predictions = model.predict([[6]])\n","print(predictions)\n","\n","2. Ridge Regression (L2 Regularization)\n","\n","Description: Ridge regression is a regularized version of linear regression. It adds a penalty to the loss function to prevent overfitting by constraining the size of the model parameters. This is done by adding the L2 norm of the coefficients to the loss function.\n","\n","Class: sklearn.linear_model.Ridge\n","\n","Example:\n","\n","\n","from sklearn.linear_model\n","\n"," import Ridge\n","\n","model = Ridge(alpha=1.0)  # Regularization strength\n","model.fit(X, y)\n","\n","predictions = model.predict([[6]])\n","\n","3. Lasso Regression (L1 Regularization)\n","\n","Description: Lasso regression is another regularized version of linear regression that uses L1 regularization. Lasso tends to produce sparse models, meaning it can drive some coefficients to zero, effectively performing feature selection.\n","\n","Class: sklearn.linear_model.Lasso\n","\n","Example:\n","\n","\n","from sklearn.linear_model\n","\n"," import Lasso\n","\n","model = Lasso(alpha=0.1)  # Regularization strength\n","model.fit(X, y)\n","\n","predictions = model.predict([[6]])"],"metadata":{"id":"QS_daE8yne4R"}},{"cell_type":"markdown","source":["The model.fit() method in Scikit-learn is used to train a machine learning model. It takes the input data (features) and the corresponding target values (labels) and fits the model to this data. The primary purpose of the fit() method is to allow the model to learn the relationship between the input features and the target values so that it can make predictions on unseen data in the future.\n","\n","For most models, model.fit() performs the following:\n","\n","Learn Parameters: It adjusts the model's internal parameters (e.g., weights and biases in a linear model) based on the training data.\n","\n","Optimize: It minimizes the loss function (or error function) using an optimization algorithm (e.g., gradient descent, or a direct solver depending on the model).\n","\n","Model Fitting: The model \"learns\" the optimal mapping from inputs (features) to outputs (targets) by fitting its parameters.\n","\n","Arguments of model.fit()\n","In most cases, model.fit() requires at least two arguments:\n","\n","X (Features):\n","\n","This is the input data or features. It typically has the shape (n_samples, n_features), where n_samples is the number of data points and n_features is the number of features (variables or columns) for each data point.\n","\n","It is often represented as a 2D array or a DataFrame (with rows as samples and columns as features).\n","\n","Type:\n","\n","Array-like: This could be a NumPy array, pandas DataFrame, or a list of lists. For example, a 2D NumPy array with shape (n_samples, n_features).\n","Example:\n","\n","\n","X = [[1, 2], [2, 3], [3, 4]]\n","\n","y (Target/Labels):\n","\n","This is the target data or labels. It contains the correct output values that the model will learn to predict.\n","\n","For regression tasks, y is typically a 1D array with real numbers.\n","\n","For classification tasks, y can be a 1D array of class labels or binary outcomes.\n","\n","Type:\n","\n","Array-like: This could be a NumPy array, pandas Series, or a list of labels. For example, a 1D NumPy array with shape (n_samples,).\n","\n","Example (Regression):\n","\n","\n","y = [2, 3, 4]  # Corresponding target values\n","\n","Example (Classification):\n","\n","\n","y = [0, 1, 0]  # Binary classification labels\n","\n","Basic Syntax:\n","\n","model.fit(X, y)\n","\n","Where:\n","\n","X: Feature matrix with shape (n_samples, n_features)\n","\n","y: Target vector with shape (n_samples,)\n","\n","Additional Optional Arguments\n","\n","Some machine learning models may accept additional optional arguments in model.fit(), depending on the type of model and specific implementation. Below are a few common optional arguments:\n","\n","sample_weight:\n","\n","This is an optional argument used to give different weights to different training samples. It allows you to specify the importance of each data point when fitting the model. This is useful when you have imbalanced classes in classification or want to emphasize certain observations in regression.\n","\n","Type: Array-like, shape (n_samples,).\n","\n","Example:\n","\n","\n","model.fit(X, y, sample_weight=[1, 0.5, 2])\n","\n","X_train and y_train for Cross-validation or Hyperparameter Tuning:\n","\n","In more advanced use cases, when performing cross-validation or hyperparameter tuning, fit() will also accept training data and corresponding targets.\n","\n","Example: In GridSearchCV or RandomizedSearchCV, fit() can take data for training and testing.\n","\n","early_stopping (for some models like GradientBoostingClassifier):\n","\n","This is an optional argument to stop training early if the model's performance on a validation set does not improve after a certain number of iterations. This is useful in models like gradient boosting, where overfitting might occur if training continues too long.\n","max_iter:\n","\n","For models like LogisticRegression, max_iter controls the number of iterations allowed for optimization. It is useful when the model is not converging within the default number of iterations.\n","solver (in models like LogisticRegression, LinearSVC):\n","\n","For models that involve optimization (e.g., logistic regression), solver specifies the algorithm used for optimization (like 'lbfgs', 'liblinear', 'saga', etc.).\n"],"metadata":{"id":"Tnc5HBDkoR0i"}},{"cell_type":"markdown","source":["The model.predict() method in Scikit-learn is used to make predictions using a trained machine learning model. After a model is trained with the fit() method, predict() is used to generate predictions (or outputs) for new, unseen data.\n","\n","The primary purpose of model.predict() is to take in input data (features) and use the model's learned parameters (e.g., coefficients in linear regression, decision boundaries in decision trees, etc.) to predict the target or output values.\n","\n","In short:\n","\n","predict() makes predictions based on the patterns the model learned during training (fit()).\n","\n","It is called after fitting the model with the training data, and it requires new data to make predictions.\n","\n","Arguments of model.predict()\n","\n","The model.predict() method generally requires one argument:\n","\n","X (Features for Prediction):\n","\n","This is the input data (or features) for which you want the model to make predictions. The input should be in the same format as the data used for training.\n","\n","The argument X must be:\n","\n","Array-like (e.g., a NumPy array, pandas DataFrame, or a list of lists).\n","It should have the same number of features (columns) as the data used in training.\n","\n","It is usually a 2D array (with shape (n_samples, n_features)), even if you want to make predictions for a single data point (in which case it would be a 2D array with just one row).\n","\n","Example:\n","\n","If the model was trained on data with 3 features, the input to predict() must also have 3 features.\n","\n","X_new = [[2, 3, 5]]  # New data point with 3 features\n","\n","predictions = model.predict(X_new)  # Make prediction for the new data point\n","\n","Example Workflow of fit() and predict()\n","\n","To understand how fit() and predict() work together, let's walk through an example of fitting a linear regression model and then making predictions.\n","\n","Step 1: Importing Necessary Libraries\n","\n","from sklearn.linear_model import LinearRegression\n","\n","import numpy as np\n","\n","Step 2: Defining the Training Data\n","\n","# Training data (features X)\n","X_train = np.array([[1], [2], [3], [4], [5]])\n","\n","# Corresponding target data (y)\n","y_train = np.array([1, 2, 3, 4, 5])\n","\n","Step 3: Creating and Training the Model (fit())\n","\n","# Create a linear regression model\n","model = LinearRegression()\n","\n","# Fit the model with the training data\n","model.fit(X_train, y_train)\n","\n","Here, model.fit() trains the model using X_train (the input features) and y_train (the target values). After training, the model will have learned the relationship between X_train and y_train.\n","\n","Step 4: Making Predictions Using New Data (predict())\n","Now, after the model is trained, you can use the predict() method to make predictions for new, unseen data:\n","\n","\n","# New data point (to make predictions)\n","X_new = np.array([[6]])\n","\n","# Make predictions for the new data point\n","predictions = model.predict(X_new)\n","\n","print(predictions)  # Output: [6.]\n","\n","The model predicts that the output for the input 6 is 6, since the relationship between X and y was learned as y = X during training."],"metadata":{"id":"mtM4H_ncpXah"}},{"cell_type":"markdown","source":["Continuous and Categorical Variables\n","\n","In the context of data analysis and machine learning, variables are often classified into two main types: continuous and categorical. Understanding these types is important because the way we handle, process, and model these variables differs. Below is a detailed explanation of each type:\n","\n","1. Continuous Variables\n","\n","A continuous variable is a type of variable that can take an infinite number of values within a given range. These variables are typically measurements that are numeric and can be expressed in decimal or fractional form. Continuous variables can represent quantities that have a natural ordering and can be divided into smaller parts.\n","\n","Characteristics of Continuous Variables:\n","\n","Numeric Values: These variables are represented by numbers, and the values can take any real value within a given range.\n","\n","Infinity of Values: Continuous variables can take an infinite number of values. For example, the height of a person can be 175.5 cm, 175.55 cm, or even 175.555 cm, and so on.\n","\n","Can Be Measured: These variables can often be measured with high precision (depending on the scale).\n","\n","Examples:\n","Temperature: Can be measured with various degrees of precision, e.g., 32.5¬∞C, 32.55¬∞C, etc.\n","\n","Height/Weight: Can take values like 167.4 cm, 167.45 cm, etc.\n","\n","Age: Can be represented as 25.5 years, 25.45 years, etc.\n","Income: $50,000.99, $50,000.50, etc.\n","\n","In summary, continuous variables can take any value within a specific range and often come from measurements or counts that involve real numbers.\n","\n","2. Categorical Variables\n","\n","A categorical variable (also called a qualitative variable) represents categories or groups. These variables describe qualitative attributes that do not have a natural order or ranking. Categorical variables take on limited values, and each value represents a category or group.\n","\n","Characteristics of Categorical Variables:\n","\n","Non-Numeric: These variables typically represent categories or groups and do not have a numeric meaning.\n","\n","Limited Number of Values: Unlike continuous variables, categorical variables have a finite number of distinct values or categories.\n","\n","Nominal vs. Ordinal: Categorical variables can be further divided into two subtypes:\n","\n","Nominal Variables: Categories with no inherent order or ranking.\n","\n","Example: Color (red, blue, green), Gender (male, female).\n","\n","Ordinal Variables: Categories with a clear, natural order or ranking.\n","\n","Example: Education level (high school, bachelor's, master's, PhD), Rating (poor, fair, good, excellent).\n","\n","Examples of Categorical Variables:\n","\n","Gender: Categories like \"Male\" and \"Female.\"\n","\n","Marital Status: Categories like \"Single\", \"Married\", \"Divorced\", \"Widowed.\"\n","\n","Color: Categories like \"Red\", \"Blue\", \"Green\".\n","\n","Education Level (Ordinal): \"High School\", \"Bachelor‚Äôs\", \"Master‚Äôs\", \"Doctorate\"\n","\n","Customer Satisfaction (Ordinal): \"Very Unsatisfied\", \"Unsatisfied\", \"Neutral\", \"Satisfied\", \"Very Satisfied\"."],"metadata":{"id":"iW2zInS6qUIW"}},{"cell_type":"markdown","source":["Feature scaling is a technique used in machine learning to standardize or normalize the range of independent variables (features) in the dataset. It ensures that each feature contributes equally to the model, preventing features with larger numerical ranges from disproportionately affecting the model‚Äôs performance.\n","\n","In simpler terms, feature scaling adjusts the values of features so that they are on a similar scale, making it easier for the model to learn the relationships between the features and the target variable.\n","\n","Feature scaling is particularly important for algorithms that are sensitive to the magnitude of the features, such as:\n","\n","Gradient Descent-based algorithms (e.g., Linear Regression, Logistic Regression, Neural Networks)\n","\n","Distance-based algorithms (e.g., K-Nearest Neighbors, Support Vector Machines, K-Means Clustering)\n","\n","Why is Feature Scaling Important in Machine Learning?\n","\n","Ensures Equal Weighting of Features:\n","\n","Without feature scaling, features with larger numerical values (e.g., income in thousands or height in centimeters) might dominate the learning process. This can make models biased toward features with larger ranges, leading to suboptimal performance.\n","Feature scaling ensures that each feature contributes equally to the model.\n","\n","Improves Convergence in Optimization:\n","\n","Many machine learning algorithms (e.g., gradient descent) rely on the optimization of a cost function (e.g., Mean Squared Error in regression). If the features are on different scales, the optimization algorithm may converge slowly or fail to converge at all.\n","Scaling the features ensures that the optimization algorithm converges more efficiently and consistently.\n","\n","Improves Performance in Distance-based Algorithms:\n","\n","Algorithms like K-Nearest Neighbors (KNN) and K-Means clustering are distance-based algorithms, meaning they calculate distances between data points. Features with larger scales will have a greater influence on the distance calculation, potentially skewing the results. Feature scaling ensures that all features contribute equally to distance calculations.\n","\n","Better Performance for Regularization:\n","\n","Regularization techniques like L2 Regularization (used in Ridge Regression or Logistic Regression) penalize large coefficients. Features that are not scaled properly might lead to larger or smaller coefficients, distorting the regularization penalty and affecting model performance.\n","\n","Facilitates Model Comparisons:\n","\n","When comparing different models (e.g., comparing a decision tree with a support vector machine), feature scaling ensures that each model interprets the features in the same way, which makes comparisons fairer and more meaningful.\n"],"metadata":{"id":"JY1yIrDTq_wK"}},{"cell_type":"code","source":["# Min-Max Scaling (Normalization)\n","from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","\n","# Sample data (2 features, 5 samples)\n","X = np.array([[1, 2],\n","              [2, 4],\n","              [3, 6],\n","              [4, 8],\n","              [5, 10]])\n","\n","# Initialize the MinMaxScaler\n","scaler = MinMaxScaler()\n","\n","# Fit the scaler and transform the data\n","X_normalized = scaler.fit_transform(X)\n","\n","print(\"Normalized Data (Min-Max Scaling):\")\n","print(X_normalized)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFJrGI_irceR","executionInfo":{"status":"ok","timestamp":1734792402281,"user_tz":-330,"elapsed":978,"user":{"displayName":"Varun Garg","userId":"11663094782443047067"}},"outputId":"9ae92061-e4cb-4bbb-ca56-eacc0ddeefcf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Normalized Data (Min-Max Scaling):\n","[[0.   0.  ]\n"," [0.25 0.25]\n"," [0.5  0.5 ]\n"," [0.75 0.75]\n"," [1.   1.  ]]\n"]}]},{"cell_type":"code","source":["# Standardization (Z-score Scaling)\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","\n","# Sample data (2 features, 5 samples)\n","X = np.array([[1, 2],\n","              [2, 4],\n","              [3, 6],\n","              [4, 8],\n","              [5, 10]])\n","\n","# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# Fit the scaler and transform the data\n","X_standardized = scaler.fit_transform(X)\n","\n","print(\"Standardized Data (Z-score Scaling):\")\n","print(X_standardized)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n3FdZycGr07X","executionInfo":{"status":"ok","timestamp":1734792443990,"user_tz":-330,"elapsed":428,"user":{"displayName":"Varun Garg","userId":"11663094782443047067"}},"outputId":"ab8f6f2f-792a-4fdb-f8e2-0c4706121528"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Standardized Data (Z-score Scaling):\n","[[-1.41421356 -1.41421356]\n"," [-0.70710678 -0.70710678]\n"," [ 0.          0.        ]\n"," [ 0.70710678  0.70710678]\n"," [ 1.41421356  1.41421356]]\n"]}]},{"cell_type":"code","source":["# Min-Max Scaling for Other Ranges\n","from sklearn.preprocessing import MinMaxScaler\n","import numpy as np\n","\n","# Sample data (2 features, 5 samples)\n","X = np.array([[1, 2],\n","              [2, 4],\n","              [3, 6],\n","              [4, 8],\n","              [5, 10]])\n","\n","# Initialize the MinMaxScaler with a custom feature range (e.g., [-1, 1])\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","\n","# Fit and transform the data\n","X_custom_range = scaler.fit_transform(X)\n","\n","print(\"Data Scaled to Range [-1, 1]:\")\n","print(X_custom_range)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wP8v8snEr_Pk","executionInfo":{"status":"ok","timestamp":1734792491869,"user_tz":-330,"elapsed":482,"user":{"displayName":"Varun Garg","userId":"11663094782443047067"}},"outputId":"3e70a586-f0d5-4cb6-ca01-38712c662f65"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Data Scaled to Range [-1, 1]:\n","[[-1.  -1. ]\n"," [-0.5 -0.5]\n"," [ 0.   0. ]\n"," [ 0.5  0.5]\n"," [ 1.   1. ]]\n"]}]},{"cell_type":"markdown","source":["sklearn.preprocessing is a module in the scikit-learn library in Python, which provides various tools and transformers for preprocessing data before using it in machine learning models. The main objective of this module is to transform and scale features in a way that improves the performance of machine learning algorithms.\n","\n","Preprocessing techniques are essential because the raw data may have inconsistent scales, missing values, non-numeric features, or outliers that can affect the performance of machine learning models. This module includes a variety of utilities for handling common data issues like scaling, encoding, and transforming features, making the data ready for machine learning algorithms.\n","\n","Key Features of sklearn.preprocessing:\n","\n","Scaling: Ensures that numerical features are on a comparable scale. This is often necessary because many machine learning algorithms are sensitive to the scale of the data (e.g., distance-based models like KNN, or gradient-based models like neural networks).\n","\n","Encoding: Converts categorical variables (non-numeric data) into a format that can be used by machine learning algorithms, such as converting strings to integers or one-hot encoding.\n","\n","Handling Missing Data: Some preprocessing tools can handle missing or NaN (Not a Number) values in the data.\n","\n","Polynomial Features: Some algorithms may benefit from higher-degree polynomial features, and the preprocessing module can generate these from existing features.\n","\n","Main Classes and Functions in sklearn.preprocessing\n","\n","Here are the most commonly used classes and functions in the sklearn.preprocessing module:\n","\n","1. Scaling Techniques\n","\n","Scaling techniques are used to adjust the range of features. This is often done to ensure that no feature dominates over others due to differences in magnitude.\n","\n","Min-Max Scaling (Normalization):\n","\n","This technique scales the feature values to a fixed range, typically between 0 and 1.\n","\n","\n","from sklearn.prepration\n","\n","scaler = MinMaxScaler()\n","\n","X_scaled = scaler.fit_transform(X)\n","\n","Standardization (Z-score Scaling):\n","\n","This technique standardizes the data by subtracting the mean and dividing by the standard deviation. It centers the data around zero and scales it so that it has a standard deviation of 1.\n","\n","\n","from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","\n","X_scaled = scaler.fit_transform(X)\n","\n","Robust Scaling:\n","\n","This scaler uses the median and interquartile range (IQR) for scaling, making it more robust to outliers compared to standardization or Min-Max scaling.\n","\n","\n","from sklearn.preprocessing import RobustScaler\n","\n","scaler = RobustScaler()\n","\n","X_scaled = scaler.fit_transform(X)\n","\n","Normalizer:\n","\n","This technique scales each individual sample (row) such that its norm (magnitude) is 1. It is useful for models that rely on vector magnitudes, such as KNN and Naive Bayes.\n","\n","\n","from sklearn.preprocessing import Normalizer\n","\n","scaler = Normalizer()\n","\n","X_scaled = scaler.fit_transform(X)\n","\n","2. Encoding Categorical Variables\n","\n","Machine learning algorithms typically require numerical data. Therefore, categorical variables need to be transformed into a numeric format.\n","\n","Label Encoding:\n","\n","Converts categorical labels (strings) into numerical integers. Label encoding is typically used when the categorical variable has an ordinal relationship (e.g., \"Low\", \"Medium\", \"High\").\n","\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","encoder = LabelEncoder()\n","\n","y_encoded = encoder.fit_transform(y)\n","\n","One-Hot Encoding:\n","\n","This method converts categorical variables into a set of binary variables (0 or 1), representing the presence of each category. This technique is typically used for nominal categorical variables where the categories have no inherent order.\n","\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","\n","encoder = OneHotEncoder()\n","\n","X_encoded = encoder.fit_transform(X)\n","\n","Note: OneHotEncoder returns a sparse matrix by default. You can convert it to a dense array using .toarray() if needed:\n","\n","\n","\n","X_encoded = encoder.fit_transform(X).toarray()\n","\n","Alternatively, for one-hot encoding in pandas, you can use pd.get_dummies().\n","\n","\n","OneHotEncoder Example with handle_unknown Parameter:\n","\n","\n","encoder = OneHotEncoder(handle_unknown='ignore')\n","\n","X_encoded = encoder.fit_transform(X)\n","\n","This will allow the encoder to handle unknown categories in the test data without throwing an error.\n","\n","3. Handling Missing Data\n","\n","Handling missing or NaN (Not a Number) values is another important aspect of preprocessing.\n","\n","Imputer:\n","\n","The SimpleImputer class can be used to replace missing values in a dataset. You can impute missing values with the mean, median, or most frequent value, or you can specify a constant value.\n","\n","from sklearn.impute import SimpleImputer\n","\n","\n","# Replace missing values with the mean\n","\n","imputer = SimpleImputer(strategy='mean')\n","\n","X_imputed = imputer.fit_transform(X)\n","\n","4. Polynomial Features\n","\n","Polynomial features are useful for creating interactions between features or higher-order terms (e.g., squared or cubic terms) in regression models. This can help capture non-linear relationships between the features and the target variable.\n","\n","\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","\n","poly = PolynomialFeatures(degree=2)\n","\n","X_poly = poly.fit_transform(X)\n","\n","This will generate new features that include the original features as well as their pairwise products, such as X1 * X2, X1^2, and X2^2.\n","\n","5. Binarizer\n","\n","This function converts numerical data into binary values (0 or 1) based on a threshold.\n","\n","\n","from sklearn.preprocessing import Binarizer\n","\n","\n","scaler = Binarizer(threshold=0.0)\n","\n","X_binarized = scaler.fit_transform(X)\n","\n","Values greater than the threshold will be transformed to 1, and values less than or equal to the threshold will be transformed to 0.\n","\n","\n","6. QuantileTransformer\n","\n","This transformer applies a non-linear transformation to map the feature values to a uniform or Gaussian distribution, which can improve the performance of certain models, such as those sensitive to outliers.\n","\n","\n","from sklearn.preprocessing import QuantileTransformer\n","\n","\n","scaler = QuantileTransformer(output_distribution='normal')\n","\n","X_transformed = scaler.fit_transform(X)\n","\n","7. FunctionTransformer\n","\n","This allows you to apply a custom function to your data. It's useful when you want to apply a specific transformation or preprocessing step.\n","\n","from sklearn.preprocessing import FunctionTransformer\n","\n","# Apply log transformation\n","\n","transformer = FunctionTransformer(np.log1p, validate=True)\n","\n","X_log_transformed = transformer.fit_transform(X)\n","\n","\n","\n"],"metadata":{"id":"xAGqqjR8sOqQ"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","import numpy as np\n","\n","# Example data: 10 samples, 2 features\n","X = np.array([[1, 2],\n","              [2, 3],\n","              [3, 4],\n","              [4, 5],\n","              [5, 6],\n","              [6, 7],\n","              [7, 8],\n","              [8, 9],\n","              [9, 10],\n","              [10, 11]])\n","\n","y = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])  # Target values\n","\n","# Split the data: 80% for training, 20% for testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Display the splits\n","print(\"Training Features (X_train):\")\n","print(X_train)\n","print(\"Test Features (X_test):\")\n","print(X_test)\n","print(\"Training Target (y_train):\")\n","print(y_train)\n","print(\"Test Target (y_test):\")\n","print(y_test)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kJVn73Lvtr0O","executionInfo":{"status":"ok","timestamp":1734792938270,"user_tz":-330,"elapsed":471,"user":{"displayName":"Varun Garg","userId":"11663094782443047067"}},"outputId":"79ad7fe4-aa62-434f-d216-ec608a698d80"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Features (X_train):\n","[[ 6  7]\n"," [ 1  2]\n"," [ 8  9]\n"," [ 3  4]\n"," [10 11]\n"," [ 5  6]\n"," [ 4  5]\n"," [ 7  8]]\n","Test Features (X_test):\n","[[ 9 10]\n"," [ 2  3]]\n","Training Target (y_train):\n","[12  2 16  6 20 10  8 14]\n","Test Target (y_test):\n","[18  4]\n"]}]},{"cell_type":"markdown","source":["Data encoding is the process of converting categorical data (which is often in the form of text or labels) into a numerical format that machine learning algorithms can understand and process. Many machine learning algorithms work only with numerical data, so categorical features need to be transformed before being fed into a model.\n","\n","There are various techniques for encoding categorical variables, depending on the type of data (nominal or ordinal) and the machine learning model you're using.\n","\n","Types of Categorical Variables\n","\n","Before diving into encoding techniques, let's distinguish between the two main types of categorical variables:\n","\n","Nominal Variables: These are categorical variables with no inherent order. Examples include colors, cities, and names. For example, the \"color\" feature might have categories like \"Red\", \"Green\", and \"Blue\", but there is no inherent order between them.\n","\n","Ordinal Variables: These categorical variables have a natural order or ranking. Examples include education levels (\"High School\", \"Bachelor's\", \"Master's\", \"PhD\") or ratings (\"Low\", \"Medium\", \"High\"). There is a clear ordering in these categories.\n","\n"],"metadata":{"id":"SShlnRzDt-KG"}}]}